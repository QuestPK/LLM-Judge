# LLM-Judge

## Prerequisites
- Ensure that `ollama` is model is running.

## Installation
1. Install the required dependencies:
    ```sh
    pip install -r requirements.txt
    ```

## Start the app
1. Flask server:
    ```sh
    python run
    ```